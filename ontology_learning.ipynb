{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYCwgrB4MzbW",
        "outputId": "0f6cbc50-0a08-4626-f462-3ffabb81273c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi1-ezkYm-CW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5u2hzNcncaP"
      },
      "outputs": [],
      "source": [
        "# !mkdir /content/drive/MyDrive/ontology\n",
        "# !cp /content/drive/MyDrive/ontology/kaggle/kaggle.json ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8x8eOUEPlaV"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ontology/kaggle/kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtssxFcsnRuD"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/ontology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1KH_ZtmzjZh"
      },
      "outputs": [],
      "source": [
        "# !mkdir arx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD7ZncZozWhp"
      },
      "outputs": [],
      "source": [
        "# !unzip arxiv.zip -d arx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJsSh1PHsBlV"
      },
      "outputs": [],
      "source": [
        "# !kaggle datasets download -d Cornell-University/arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ2vTYEpM2VP"
      },
      "source": [
        "Data analysis and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBLHUDl5M4Vj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import logging\n",
        "import dask.bag as db\n",
        "from typing import Generator, List, Tuple, Optional, Any\n",
        "import spacy\n",
        "import nltk\n",
        "import itertools\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "from nltk import FreqDist\n",
        "\n",
        "logger = logging.getLogger(\"spacy\")\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6TEY6nRNDrA"
      },
      "outputs": [],
      "source": [
        "# docs = db.read_text('arx/arxiv-metadata-oai-snapshot.json').map(json.loads)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZSj4QqaNz-k"
      },
      "outputs": [],
      "source": [
        "# docs.count().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga20iwOAHxXu"
      },
      "outputs": [],
      "source": [
        "PATH_ = \"arx/arxiv-metadata-oai-snapshot.json\"\n",
        "NUM_PAPERS = 5000\n",
        "\n",
        "def get_dataset_generator(path: str) -> Generator:\n",
        "    with open(path, \"r\") as fp:\n",
        "        for line in fp:\n",
        "            row = json.loads(line)\n",
        "            yield row\n",
        "\n",
        "dataset_generator = get_dataset_generator(\n",
        "    path=PATH_\n",
        ")\n",
        "print(type(dataset_generator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbh-dyE6Moyz"
      },
      "outputs": [],
      "source": [
        "def create_dataframe(generator: Generator) -> pd.DataFrame:\n",
        "    # I'll use this column to filter out paper duplicates.\n",
        "    titles = []\n",
        "    authors = []\n",
        "\n",
        "    abstracts = []\n",
        "    categories = []\n",
        "    dates = []\n",
        "\n",
        "    for row in generator:\n",
        "        if len(abstracts) == NUM_PAPERS:\n",
        "            break\n",
        "\n",
        "        titles.append(row[\"title\"])\n",
        "        authors.append(row[\"authors\"])\n",
        "\n",
        "        dates.append(row[\"update_date\"])\n",
        "        abstracts.append(row[\"abstract\"])\n",
        "        categories.append(row[\"categories\"])\n",
        "\n",
        "    return pd.DataFrame.from_dict({\n",
        "        \"title\": titles,\n",
        "        \"authors\": authors,\n",
        "        \"date\": dates,\n",
        "        \"abstract\": abstracts,\n",
        "        \"categories\": categories\n",
        "    })\n",
        "\n",
        "\n",
        "dataset_df = create_dataframe(dataset_generator)\n",
        "dataset_df[\"date\"] = pd.to_datetime(dataset_df[\"date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI3yOU16NBHq"
      },
      "outputs": [],
      "source": [
        "dataset_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LuTPpPHNDEE"
      },
      "outputs": [],
      "source": [
        "dataset_df.loc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VolyedA18dAq"
      },
      "outputs": [],
      "source": [
        "df = dataset_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsKahrL80N_"
      },
      "source": [
        "#Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jzBcY6c82_k"
      },
      "source": [
        "## 1.1 Stopword filtering using spaCy and NLTK stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qPfaZNd82Eg"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "df['abstract'] = df['abstract'].apply(lambda text: \" \".join([word for word in text.split() if word.lower() not in stop_words]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfbKewg_9EkC"
      },
      "source": [
        "## 1.2 POS tagging using huggingface pretrained POS tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNGLZ5rE9DmV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.do_word_tokenize = True\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "pos_tagger = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "df['abstract_tokens'] = df['abstract'].apply(lambda text: pos_tagger(text))\n",
        "df['word'] = df['abstract_tokens'].apply(lambda tokens: [token['word'] for token in tokens])\n",
        "df['pos_token'] = df['abstract_tokens'].apply(lambda tokens: [token['entity'] for token in tokens])\n",
        "df = df.explode('word').explode('pos_token')\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYYqagRIO-S4"
      },
      "outputs": [],
      "source": [
        "df.to_csv('tokenized_df_dump.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X22jhYcV9FOC"
      },
      "source": [
        "## 1.3. Frequency analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JCkADko9GJz"
      },
      "outputs": [],
      "source": [
        "all_words = [word for tokens in df['abstract_tokens'] for word in tokens]\n",
        "fdist = FreqDist(all_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a69G9Z99GzI"
      },
      "source": [
        "## 1.4. Key terms extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92oY8JH69QhN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "\n",
        "df['word_frequencies'] = df.groupby('word')['word'].transform('count')\n",
        "\n",
        "whole_dataset_word_counts = Counter([word for text in df['abstract'] for word in text.split()])\n",
        "df['word_frequencies_in_dataset'] = df['word'].map(whole_dataset_word_counts)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['abstract'])\n",
        "df['tfidf'] = [tfidf_matrix[i].toarray()[0] for i in range(len(df))]\n",
        "\n",
        "df['keywords'] = df.apply(lambda row: [word for word, tfidf_score in zip(row['word'].split(), row['tfidf']) if tfidf_score > 0.5], axis=1)\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HASTI"
      ],
      "metadata": {
        "id": "FkKFs8yCgL0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Extract synonyms using WordNet and Wiktionary\n"
      ],
      "metadata": {
        "id": "W1LwzXO-hP_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import spacy\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = []\n",
        "\n",
        "    for synset in wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "\n",
        "    return synonyms\n",
        "\n"
      ],
      "metadata": {
        "id": "kN2hDc8RgNfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Calculating Relations with context similarity between concepts\n"
      ],
      "metadata": {
        "id": "P9dJ1aw0hZKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(concept1, concept2):\n",
        "    doc1 = nlp(concept1)\n",
        "    doc2 = nlp(concept2)\n",
        "    similarity = doc1.similarity(doc2)\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "4Aj4g7KWhaN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Extract various pre-versions of Domain Corpuses based on different thresholds\n"
      ],
      "metadata": {
        "id": "HQnFBlANhfoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_corpus_with_threshold(data, threshold):\n",
        "    corpus = defaultdict(list)\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        abstract = row['abstract']\n",
        "        keywords = row['keywords']\n",
        "\n",
        "        for keyword in keywords:\n",
        "            synonyms = get_synonyms(keyword)\n",
        "\n",
        "            for syn in synonyms:\n",
        "                similarity = calculate_similarity(keyword, syn)\n",
        "\n",
        "                # Extract concepts based on threshold similarity\n",
        "                if similarity >= threshold:\n",
        "                    corpus[keyword].append(abstract)\n",
        "\n",
        "    return corpus\n"
      ],
      "metadata": {
        "id": "jJEZfI7qhh17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpuses = []\n",
        "for i in range(10):\n",
        "  threshold = i/10\n",
        "  corpuses.append(extract_corpus_with_threshold(df, threshold))"
      ],
      "metadata": {
        "id": "5n_a5ZwnhvHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1-2 Calculate entropy and information gain between different pre versions of Domain Corpuses and choose most appropriate Corpus and Calculate information gain during feeding different documents to ensure\n",
        "viability of the current corpus"
      ],
      "metadata": {
        "id": "Jb9pyhK9iawf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy(corpus):\n",
        "    total_documents = sum(len(docs) for docs in corpus.values())\n",
        "    entropy = 0.0\n",
        "\n",
        "    for concept, docs in corpus.items():\n",
        "        probability = len(docs) / total_documents\n",
        "        entropy -= probability * np.log2(probability)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(corpus_before, corpus_after):\n",
        "    entropy_before = calculate_entropy(corpus_before)\n",
        "    entropy_after = calculate_entropy(corpus_after)\n",
        "    information_gain = entropy_before - entropy_after\n",
        "    return information_gain\n",
        "\n",
        "\n",
        "def calculate_all_information_gains(corpuses):\n",
        "    all_information_gains = []\n",
        "    permutations = itertools.permutations(corpuses, 2)\n",
        "\n",
        "    for corpus1, corpus2 in permutations:\n",
        "        information_gain = calculate_information_gain(corpus1, corpus2)\n",
        "        all_information_gains.append((corpus1, corpus2, information_gain))\n",
        "\n",
        "    return all_information_gains\n",
        "\n",
        "all_information_gains = np.array(calculate_all_information_gains(corpuses))\n",
        "\n",
        "corpus_mvp = corpuses[np.argmax(all_information_gains,axis=0)]\n"
      ],
      "metadata": {
        "id": "QilVlzTziUNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Based on hierarchical clustering evaluate different concept groups\n"
      ],
      "metadata": {
        "id": "ghPddSprjzjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "def hierarchical_clustering(corpus, num_clusters):\n",
        "    clustering = AgglomerativeClustering(n_clusters=num_clusters, linkage='ward')\n",
        "    cluster_labels = clustering.fit_predict(corpus)\n",
        "    return cluster_labels\n",
        "\n",
        "num_clusters = 5\n",
        "cluster_labels = hierarchical_clustering(corpus, num_clusters)\n",
        "\n",
        "for i, label in enumerate(cluster_labels):\n",
        "    G.add_node(i, cluster=label)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qlpxOnrDjd2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # 4.2. Calculate most related words to this concept hierarchy level\n"
      ],
      "metadata": {
        "id": "zYo5RaWqj44b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_related_words(cluster_label, corpus, threshold):\n",
        "    related_words = []\n",
        "\n",
        "    for word, docs in corpus.items():\n",
        "        word_cluster_labels = [G.nodes[i]['cluster'] for i in range(len(corpus[word]))]\n",
        "\n",
        "        if cluster_label in word_cluster_labels:\n",
        "            cluster_count = word_cluster_labels.count(cluster_label)\n",
        "            total_count = len(word_cluster_labels)\n",
        "            cluster_proportion = cluster_count / total_count\n",
        "\n",
        "            if cluster_proportion >= threshold:\n",
        "                related_words.append(word)\n",
        "\n",
        "    return related_words\n",
        "\n",
        "target_cluster_label = 0  #\n",
        "threshold = 0.5\n",
        "related_words = calculate_related_words(target_cluster_label, corpus, threshold)\n",
        "print(f\"Related words for cluster {target_cluster_label}: {related_words}\")"
      ],
      "metadata": {
        "id": "wa0-wO0_j5je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROMINE"
      ],
      "metadata": {
        "id": "Nose-vo9j_GS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCJORiCSkthM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1; 2.3. Extract features from document, sentence, and word level\n"
      ],
      "metadata": {
        "id": "WJIfSGi9j_JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "abstracts = df['abstract']\n",
        "\n",
        "document_features = []\n",
        "sentence_features = []\n",
        "word_features = []\n",
        "\n",
        "for abstract in abstracts:\n",
        "    doc = nlp(abstract)\n",
        "\n",
        "    doc_features = {\n",
        "        \"num_sentences\": len(list(doc.sents)),\n",
        "        \"num_words\": len(doc),\n",
        "    }\n",
        "    document_features.append(doc_features)\n",
        "\n",
        "    sentence_features_per_abstract = []\n",
        "    for sent in doc.sents:\n",
        "        sent_features = {\n",
        "            \"num_tokens\": len(sent),\n",
        "            \"sentence_text\": sent.text,\n",
        "        }\n",
        "        sentence_features_per_abstract.append(sent_features)\n",
        "    sentence_features.append(sentence_features_per_abstract)\n",
        "\n",
        "    word_features_per_abstract = []\n",
        "    for token in doc:\n",
        "        word_features = {\n",
        "            \"word_text\": token.text,\n",
        "            \"word_pos\": token.pos_,\n",
        "            \"word_dep\": token.dep_,\n",
        "        }\n",
        "        word_features_per_abstract.append(word_features)\n",
        "    word_features.append(word_features_per_abstract)\n",
        "\n"
      ],
      "metadata": {
        "id": "KuX4EPJukuQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2. Provide dataset based on the word level, but containing upper structure as attributes\n"
      ],
      "metadata": {
        "id": "s7pNMeNrk1IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_level_dataset = []\n",
        "for i, abstract in enumerate(abstracts):\n",
        "    for j, token_features in enumerate(word_features[i]):\n",
        "        word_level_dataset.append({\n",
        "            \"abstract_text\": abstract,\n",
        "            \"sentence_text\": sentence_features[i][j][\"sentence_text\"],\n",
        "            \"word_text\": token_features[\"word_text\"],\n",
        "            \"word_pos\": token_features[\"word_pos\"],\n",
        "            \"word_dep\": token_features[\"word_dep\"],\n",
        "        })\n",
        "\n",
        "word_level_df = pd.DataFrame(word_level_dataset)\n"
      ],
      "metadata": {
        "id": "u_-LlxGok2YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Based on input-ontology apply current rules on the sentence structures and morphological analysis"
      ],
      "metadata": {
        "id": "sFuoXLRAlNcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_conceptual_relational_knowledge(sentence, ontology):\n",
        "        for concept, synonyms in ontology.items():\n",
        "            if word in synonyms:\n",
        "                knowledge.append((concept, word))\n",
        "    return knowledge\n",
        "\n",
        "extracted_knowledge = []\n",
        "for abstract in df['abstract']:\n",
        "    doc = nlp(abstract)\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        sentence_text = sentence.text\n",
        "        knowledge = extract_conceptual_relational_knowledge(sentence_text, ontology)\n",
        "        extracted_knowledge.append((sentence_text, knowledge))\n",
        "\n"
      ],
      "metadata": {
        "id": "POishoqhlL9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Create pre-ontologic-concepts (Ontels) based on this analysis\n"
      ],
      "metadata": {
        "id": "2jpuZaCdlr2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ontels = {}\n",
        "for sentence, knowledge in extracted_knowledge:\n",
        "    for concept, word in knowledge:\n",
        "        if concept not in ontels:\n",
        "            ontels[concept] = set()\n",
        "        ontels[concept].add(word)\n",
        "\n",
        "for concept, words in ontels.items():\n",
        "    print(f\"Ontel: {concept}\")\n",
        "    print(f\"Words: {', '.join(words)}\")\n",
        "    break()"
      ],
      "metadata": {
        "id": "kV-nRvk7lMBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Apply lexicon manager"
      ],
      "metadata": {
        "id": "Mr-8SmEMl74J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_lexicon_manager(ontology, new_terms):\n",
        "    # Add new terms to the ontology or update existing synonyms\n",
        "    for concept, synonyms in new_terms.items():\n",
        "        if concept not in ontology:\n",
        "            ontology[concept] = []\n",
        "        ontology[concept].extend(synonyms)\n",
        "\n",
        "apply_lexicon_manager(ontology, new_terms_to_add)"
      ],
      "metadata": {
        "id": "snh5DZBJl8yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Apply ontology manager (is not provided here yet)"
      ],
      "metadata": {
        "id": "4Hr7LjiTmCJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Updated Ontology:\")\n",
        "for concept, synonyms in ontology.items():\n",
        "    print(f\"Concept: {concept}, Synonyms: {', '.join(synonyms)}\")\n",
        "    break()"
      ],
      "metadata": {
        "id": "KUf-8wRlmF_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}